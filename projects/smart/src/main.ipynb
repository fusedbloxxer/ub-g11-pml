{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os as so\n",
    "import numpy as ny\n",
    "from os import path\n",
    "import pandas as ps\n",
    "import pathlib as pb\n",
    "from typing import List\n",
    "from modules import models\n",
    "import matplotlib.pyplot as pt\n",
    "from modules import processing as pg\n",
    "from numpy.random import default_rng\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from torch.optim import lr_scheduler as sch_lr\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import KFold\n",
    "from modules.blocks import Swish\n",
    "from numpy.random import default_rng\n",
    "import modules.params as params\n",
    "import copy\n",
    "import torch\n",
    "import typing\n",
    "\n",
    "\n",
    "# Configure the paths to the data files\n",
    "ROOT_PATH = pb.Path('..')\n",
    "DATASET_PATH = pb.Path(path.join(ROOT_PATH, 'data'))\n",
    "DATASET_TEST_PATH = pb.Path(path.join(DATASET_PATH, 'test'))\n",
    "DATASET_TRAIN_PATH = pb.Path(path.join(DATASET_PATH, 'train'))\n",
    "SUBMISSIONS_PATH = pb.Path(path.join(ROOT_PATH, 'submissions'))\n",
    "DATASET_TRAIN_LABELS_FILEPATH = pb.Path(path.join(DATASET_PATH, 'train_labels.csv'))\n",
    "\n",
    "# Use GPU if available\n",
    "so.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tw\n",
    "\n",
    "# Show warning if running on CPU\n",
    "available_devices = list(map(lambda d: d.device_type, tw.config.list_physical_devices()))\n",
    "print('Available devices = ', tw.config.list_physical_devices())\n",
    "\n",
    "if 'GPU' not in available_devices:\n",
    "  print('Warning: running on CPU only')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from disk\n",
    "dataset = pg.Dataset(DATASET_PATH)\n",
    "\n",
    "# Compute sizes in order to search for potential missing values in each time series\n",
    "train_sizes = [sample.shape[0] for sample in dataset.train_data]\n",
    "test_sizes = [sample.shape[0] for sample in dataset.train_data]\n",
    "\n",
    "# Display missing value stats\n",
    "pt.figure()\n",
    "for i, (sizes, title) in enumerate([(train_sizes, 'train'), (test_sizes, 'test')]):\n",
    "  pt.subplot(1, 2, i + 1)\n",
    "  pt.title(f'Missing values in {title.capitalize()}')\n",
    "  pt.xlabel('Time Series Size')\n",
    "  pt.ylabel('Frequency')\n",
    "  pt.hist(sizes)\n",
    "  pt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove outliers\n",
    "# dataset.remove_outliers(by='global', factor=4)\n",
    "\n",
    "# # Compute sizes in order to search for potential missing values in each time series\n",
    "# train_sizes = [sample.shape[0] for sample in dataset.train_data]\n",
    "# test_sizes = [sample.shape[0] for sample in dataset.train_data]\n",
    "\n",
    "# # Display missing value stats\n",
    "# pt.figure()\n",
    "# for i, (sizes, title) in enumerate([(train_sizes, 'train'), (test_sizes, 'test')]):\n",
    "#   pt.subplot(1, 2, i + 1)\n",
    "#   pt.title(f'Missing values in {title.capitalize()}')\n",
    "#   pt.xlabel('Time Series Size')\n",
    "#   pt.ylabel('Frequency')\n",
    "#   pt.hist(sizes)\n",
    "#   pt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill holes using interpolation\n",
    "dataset.fill_gaps(n_size=150, min_limit=None)\n",
    "\n",
    "# Compute sizes in order to search for potential missing values in each time series\n",
    "train_sizes = [sample.shape[0] for sample in dataset.train_data]\n",
    "test_sizes = [sample.shape[0] for sample in dataset.test_data]\n",
    "\n",
    "# Display missing value stats\n",
    "pt.figure()\n",
    "for i, (sizes, title) in enumerate([(train_sizes, 'train'), (test_sizes, 'test')]):\n",
    "  pt.subplot(1, 2, i + 1)\n",
    "  pt.title(f'Filled gaps in {title.capitalize()}')\n",
    "  pt.xlabel('Time Series Size')\n",
    "  pt.ylabel('Frequency')\n",
    "  pt.hist(sizes)\n",
    "  pt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom data preprocessing before normalization\n",
    "dataset.preprocess(rounding=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample three recordings\n",
    "gen = default_rng(24)\n",
    "sample_i = gen.choice(9000, size=1, replace=False)\n",
    "\n",
    "# Try different normalization / feature scaling methods\n",
    "strategies = (\n",
    "  (None, None),         # => no operation is applied    => -\n",
    "  ('standard', (0, 1)), # => for each coordiante        => 3\n",
    "  ('standard', (0,)),   # => temporal * each coordiante => 150 x 3\n",
    "  ('min_max', (0, 1)),  # => for each coordiante        => 3\n",
    "  ('min_max', (0,)),    # => temporal * each coordiante => 150 x 3\n",
    "  ('normalize', (1,)),  # => norm of each coordinate    => 9000 x 3\n",
    "  ('normalize', (2,)),  # => norm of each time moment   => 9000 x 150\n",
    ")\n",
    "\n",
    "# Display a sample in both sensor space and normalized space\n",
    "f = pt.figure(figsize=(15, 5))\n",
    "f.suptitle(f'User {i}')\n",
    "for i, (strategy, axis) in enumerate(strategies):\n",
    "  # Normalize copy to view different perspectives\n",
    "  dataset_copy = copy.deepcopy(dataset)\n",
    "  dataset_copy.normalize(strategy, axis)\n",
    "\n",
    "  # Retrieve normalized samples\n",
    "  record_x = dataset_copy.train_data[sample_i][0]\n",
    "\n",
    "  # Display scaled vectors\n",
    "  plot_n = f.add_subplot(2, len(strategies), i + 1, projection='3d')\n",
    "  plot_n.plot(record_x[:, 0], record_x[:, 1], record_x[:, 2])\n",
    "  plot_n.set_title(f'{strategy} {axis}')\n",
    "  plot_n.view_init(elev=40, azim=30)\n",
    "  del dataset_copy\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the training & testing data\n",
    "dataset.normalize('standard', (0,))\n",
    "\n",
    "# Reshape to (n_samples, n_features) commonly used format\n",
    "dataset.reshape((-1, 450))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model providers / factories\n",
    "model_svm_factory = lambda: models.SVMModel({\n",
    "  'C': 7,\n",
    "  'gamma': 0.01,\n",
    "  'kernel': 'rbf',\n",
    "}, verbose=False)\n",
    "\n",
    "model_knn_factory = lambda: models.KNNModel({\n",
    "  'n_neighbors': 8,\n",
    "  'p': 2,\n",
    "})\n",
    "\n",
    "model_boosted_trees_factory = lambda: models.BoostedTreesModel({\n",
    "  'learning_rate': 0.4,\n",
    "  'n_estimators': 20,\n",
    "  'subsample': 0.5,\n",
    "  'max_depth': 2,\n",
    "}, verbose = 1)\n",
    "\n",
    "model_tcnn_factory = lambda: models.TCNNModel({\n",
    "  'optim': lambda: Adam(learning_rate=2e-4, weight_decay=2e-4),\n",
    "  'm_init': tw.keras.initializers.GlorotNormal(seed=24),\n",
    "  'activ_fn': tw.keras.layers.Activation('leaky_relu'),\n",
    "  'lr_update': (30, 10, 0.8),\n",
    "  'n_filters': 128,\n",
    "  's_filters': 3,\n",
    "  'n_units': 1024,\n",
    "  'dropout': 0.35,\n",
    "  'n_epochs': 200,\n",
    "  'n_batch': 32,\n",
    "}, ROOT_PATH)\n",
    "\n",
    "model_attention_tcnn_factory = lambda: models.AttentionTCNNModel({\n",
    "  'sch_lr': lambda o, v: sch_lr.ReduceLROnPlateau(o, 'min', verbose=v, patience=10, cooldown=5),\n",
    "  'optim': lambda params: torch.optim.AdamW(params, lr=2e-4, weight_decay=6e-4),\n",
    "  'init_fn': lambda w: torch.nn.init.xavier_normal_(w),\n",
    "  'activ_fn': Swish,\n",
    "  'bottleneck': 8,\n",
    "  'dropout': 0.2,\n",
    "  's_filters': 3,\n",
    "  'n_filters': 256,\n",
    "  'n_epochs': 100,\n",
    "  'n_units': 1024,\n",
    "  'n_batch': 32,\n",
    "  'norm': True,\n",
    "  'bias': True,\n",
    "}, device=device, verbose=True)\n",
    "\n",
    "def hybrid(cl_model_factory: typing.Callable[[], models.Model]):\n",
    "  ae_model = models.AutoEncoderModel({\n",
    "    'sch_lr': lambda o, v: sch_lr.ReduceLROnPlateau(o, 'min', verbose=v, patience=10, cooldown=5),\n",
    "    'optim': lambda params: torch.optim.Adam(params, lr=1e-3),\n",
    "    'init_fn': lambda w: torch.nn.init.xavier_normal_(w),\n",
    "    'loss_fn': torch.nn.MSELoss(reduction='none'),\n",
    "    'activ_fn': torch.nn.LeakyReLU,\n",
    "    'embedding_features': 128,\n",
    "    'n_filters': 16,\n",
    "    'dropout': 0.0,\n",
    "    'n_epochs': 25,\n",
    "    'n_batch': 32,\n",
    "    'bias': True,\n",
    "  }, device=torch.device('cuda'), verbose=True)\n",
    "  return models.HybridAutoEncoderClassifier(ae_model, cl_model_factory())\n",
    "model_hybrid_factory = lambda: hybrid(model_svm_factory)\n",
    "\n",
    "# Setup current model to be used\n",
    "model_factory = model_tcnn_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display periodic plots across each fold\n",
    "pt.figure()\n",
    "\n",
    "# Configure kfold loop\n",
    "tparams: params.TrainParams = {\n",
    "  'n_folds': 5,\n",
    "}\n",
    "\n",
    "# Train and validate using crossvalidation technique\n",
    "trn_accy, val_accy = [], []\n",
    "for i, (fold_train, fold_valid) in enumerate(KFold(shuffle=True, n_splits=tparams['n_folds']).split(dataset.train_data, dataset.train_labels)):\n",
    "  # Indicate current iteration\n",
    "  print(f\"{tparams['n_folds']}-Fold: {i + 1}\")\n",
    "\n",
    "  # Create new model with random weights\n",
    "  model = model_factory()\n",
    "\n",
    "  # Gather points for each step\n",
    "  train_samples_fold = dataset.train_data[fold_train.tolist()]\n",
    "  valid_samples_fold = dataset.train_data[fold_valid.tolist()]\n",
    "  train_labels_fold = dataset.train_labels[fold_train.tolist()]\n",
    "  valid_labels_fold = dataset.train_labels[fold_valid.tolist()]\n",
    "\n",
    "  # Learn on training data and predict on valid set\n",
    "  history = model.fit(train_samples_fold, train_labels_fold, \\\n",
    "                      valid_samples_fold, valid_labels_fold)\n",
    "\n",
    "  # Save max valid & train accy for the current fold\n",
    "  val_accy.append(history.accuracy[1])\n",
    "  trn_accy.append(history.accuracy[0])\n",
    "\n",
    "  # Display fold results\n",
    "  history.show()\n",
    "\n",
    "# Show results across all folds\n",
    "pt.show()\n",
    "pt.figure()\n",
    "print(f'mean_val_accy = {ny.mean(val_accy)}')\n",
    "print(f'mean_trn_accy = {ny.mean(trn_accy)}')\n",
    "\n",
    "# Show val across folds\n",
    "pt.subplot(1, 2, 1)\n",
    "pt.grid(True)\n",
    "pt.xlabel('nth Fold')\n",
    "pt.ylabel('Accuracy')\n",
    "pt.title('Validation Accuracy')\n",
    "pt.plot(ny.arange(tparams['n_folds']), val_accy)\n",
    "\n",
    "# Show train across folds\n",
    "pt.subplot(1, 2, 2)\n",
    "pt.grid(True)\n",
    "pt.xlabel('nth Fold')\n",
    "pt.ylabel('Accuracy')\n",
    "pt.title('Training Accuracy')\n",
    "pt.plot(ny.arange(tparams['n_folds']), trn_accy)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate model to train from scratch for subtask\n",
    "model = model_factory()\n",
    "\n",
    "# Split data indices\n",
    "rnd_indices = gen.choice(len(dataset.train_data), size=len(dataset.train_data), replace=False)\n",
    "n_subset_valid = 2_000\n",
    "n_subset_train_i = dataset.train_data.shape[0] - n_subset_valid\n",
    "n_subset_valid_i = n_subset_train_i\n",
    "\n",
    "# Fit model on all training data except last 2_000 entries\n",
    "model.fit(dataset.train_data[rnd_indices[:n_subset_train_i]], dataset.train_labels[rnd_indices[:n_subset_train_i]])\n",
    "\n",
    "# Predict on last 2_000 entries\n",
    "m_cnf = model.conf(dataset.train_data[rnd_indices[n_subset_valid_i:]], dataset.train_labels[rnd_indices[n_subset_valid_i:]])\n",
    "\n",
    "# Transform back to labels instead of probs\n",
    "f = pt.figure()\n",
    "a = f.add_subplot()\n",
    "a.set_title(f'Validation Accuracy: {m_cnf.trace() / m_cnf.sum()}')\n",
    "ConfusionMatrixDisplay(m_cnf).plot(ax=a)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model for final prediction on test set\n",
    "model = model_factory()\n",
    "\n",
    "# Fit on the entire training to have more data for further predictions\n",
    "rnd_indices = gen.choice(len(dataset.train_data), size=len(dataset.train_data), replace=False)\n",
    "history = model.fit(dataset.train_data[rnd_indices], dataset.train_labels[rnd_indices])\n",
    "print(history.accuracy[0])\n",
    "history.show()\n",
    "\n",
    "# Predict unknown labels\n",
    "test_pred_labels = model.predict(dataset.test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resulting test object\n",
    "test_ids = ps.Series([p.stem for p in sorted(DATASET_TEST_PATH.glob('*.csv'))], name='id')\n",
    "test_classes = ps.Series(test_pred_labels, name='class')\n",
    "test_results = ps.DataFrame({\n",
    "  'id': test_ids,\n",
    "  'class': test_classes,\n",
    "})\n",
    "\n",
    "# And store the results obtained by the model using that previously created test frame\n",
    "test_results.to_csv(SUBMISSIONS_PATH / 'test_labels_36.csv', mode='w', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "smart-lIfyssvW-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 (main, Nov 14 2022, 16:10:14) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e565ec849dcddaec7b38b9ef97cfe3fbbc1ae0f4c9cc1bfd371c87a84ea0feaa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
