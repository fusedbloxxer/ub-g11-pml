{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-27 12:38:18.037291: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-27 12:38:18.151008: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-11-27 12:38:18.632761: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cudnn/lib64:\n",
      "2022-11-27 12:38:18.632797: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cudnn/lib64:\n",
      "2022-11-27 12:38:18.632800: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os as so\n",
    "import numpy as ny\n",
    "from os import path\n",
    "import pandas as ps\n",
    "import pathlib as pb\n",
    "from typing import List\n",
    "import matplotlib.pyplot as pt\n",
    "from modules import processing as pg\n",
    "from numpy.random import default_rng\n",
    "\n",
    "\n",
    "# Use GPU if available\n",
    "so.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices =  [PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'), PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tw\n",
    "from tensorflow import Tensor\n",
    "\n",
    "# Show warning if running on CPU\n",
    "available_devices = list(map(lambda d: d.device_type, tw.config.list_physical_devices()))\n",
    "print('Available devices = ', tw.config.list_physical_devices())\n",
    "\n",
    "if 'GPU' not in available_devices:\n",
    "  print('Warning: running on CPU only')\n",
    "\n",
    "# Configure the paths to the data files\n",
    "ROOT_PATH = pb.Path('..')\n",
    "DATASET_PATH = pb.Path(path.join(ROOT_PATH, 'data'))\n",
    "DATASET_TEST_PATH = pb.Path(path.join(DATASET_PATH, 'test'))\n",
    "DATASET_TRAIN_PATH = pb.Path(path.join(DATASET_PATH, 'train'))\n",
    "SUBMISSIONS_PATH = pb.Path(path.join(ROOT_PATH, 'submissions'))\n",
    "DATASET_TRAIN_LABELS_FILEPATH = pb.Path(path.join(DATASET_PATH, 'train_labels.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train labels raw and use them for lookup\n",
    "train_labels_lookup = ps.read_csv(DATASET_TRAIN_LABELS_FILEPATH)\n",
    "\n",
    "# Fetch eatch train sample and do lookup for the corresponding label\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for sample_path in sorted(DATASET_TRAIN_PATH.glob('*.csv')):\n",
    "  # Lookup the label\n",
    "  train_label = train_labels_lookup[train_labels_lookup['id'] == int(sample_path.stem)]\n",
    "  train_labels.append(train_label['class'].item())\n",
    "\n",
    "  # Read the sample and convert it to a tf compatible format\n",
    "  train_entry = ps.read_csv(sample_path, header=None, names=['x', 'y', 'z'])\n",
    "  train_data.append(tw.convert_to_tensor(train_entry))\n",
    "\n",
    "print('Number of training samples: ', len(train_data))\n",
    "print('Shape of a training sample: ', train_data[0].shape)\n",
    "\n",
    "# Read test samples\n",
    "test_data = []\n",
    "for sample_path in sorted(DATASET_TEST_PATH.glob('*.csv')):\n",
    "  test_entry = ps.read_csv(sample_path, header=None, names=['x', 'y', 'z'])\n",
    "  test_data.append(tw.convert_to_tensor(test_entry))\n",
    "\n",
    "print('Number of testing samples: ', len(test_data))\n",
    "print('Shape of a testing sample: ', test_data[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sizes in order to search for potential missing values in each time series\n",
    "train_sizes = [sample.shape[0] for sample in train_data]\n",
    "test_sizes = [sample.shape[0] for sample in test_data]\n",
    "\n",
    "# Display missing value stats\n",
    "pt.figure()\n",
    "for i, (sizes, title) in enumerate([(train_sizes, 'train'), (test_sizes, 'test')]):\n",
    "  pt.subplot(1, 2, i + 1)\n",
    "  pt.title(f'Missing values in {title.capitalize()}')\n",
    "  pt.xlabel('Time Series Size')\n",
    "  pt.ylabel('Frequency')\n",
    "  pt.hist(sizes)\n",
    "  pt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing gaps and drop additional points\n",
    "train_data_proc = pg.fill_missing_values(train_data, 150)\n",
    "test_data_proc = pg.fill_missing_values(test_data, 150)\n",
    "\n",
    "# Compute sizes in order to search for potential missing values in each time series\n",
    "train_sizes = [sample.shape[0] for sample in train_data_proc]\n",
    "test_sizes = [sample.shape[0] for sample in test_data_proc]\n",
    "\n",
    "# Display missing value stats\n",
    "pt.figure()\n",
    "for i, (sizes, title) in enumerate([(train_sizes, 'train'), (test_sizes, 'test')]):\n",
    "  pt.subplot(1, 2, i + 1)\n",
    "  pt.title(f'Filled gaps in {title.capitalize()}')\n",
    "  pt.xlabel('Time Series Size')\n",
    "  pt.ylabel('Frequency')\n",
    "  pt.hist(sizes)\n",
    "  pt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the mean and std over the training data across the temporal dimension\n",
    "train_mean = ny.mean(train_data_proc, axis=(0, 1))\n",
    "train_std = ny.std(train_data_proc, axis=(0, 1))\n",
    "print('train_mean = ', train_mean)\n",
    "print('train_std = ', train_std)\n",
    "\n",
    "# Scale the features for both training and testing\n",
    "train_data_norm = (train_data_proc - train_mean) / train_std\n",
    "test_data_norm = (test_data_proc - train_mean) / train_std\n",
    "\n",
    "# Reshape the input for the network\n",
    "train_data_in = tw.expand_dims(train_data_norm, axis=1)\n",
    "test_data_in = tw.expand_dims(test_data_norm, axis=1)\n",
    "\n",
    "# Map test labels to [0, 19] and encode them using one-hot\n",
    "train_labels_out = ny.array(train_labels) - 1\n",
    "train_labels_one_hot = tw.raw_ops.OneHot(indices=train_labels_out, depth=20, on_value=1.0, off_value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nvitop.callbacks.keras import GpuStatsLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from typing import TypedDict, Callable\n",
    "\n",
    "class TrainParams(TypedDict):\n",
    "  n_folds: int\n",
    "  n_batch: int\n",
    "  n_epochs: int\n",
    "\n",
    "class TCNNParams(TypedDict):\n",
    "  optim: Callable[[], tw.keras.optimizers.Optimizer]\n",
    "  m_init: tw.keras.initializers.VarianceScaling\n",
    "  activ_fn: tw.keras.layers.Activation\n",
    "  n_filters: int\n",
    "  dropout: float\n",
    "  n_units: int\n",
    "\n",
    "def create_tcnn_model(hparams: TCNNParams) -> tw.keras.Model:\n",
    "  model_tcnn = tw.keras.Sequential([\n",
    "    tw.keras.layers.InputLayer(input_shape=(1, 150, 3)),\n",
    "\n",
    "    tw.keras.layers.Conv2D(hparams['n_filters'], kernel_size=(1, hparams['s_filters']), padding='same', kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.Conv2D(hparams['n_filters'], kernel_size=(1, hparams['s_filters']), padding='same', kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.MaxPool2D(pool_size=(1, 3), strides=(1, 2), padding='same'),\n",
    "    tw.keras.layers.Dropout(hparams['dropout']),\n",
    "\n",
    "    tw.keras.layers.Conv2D(hparams['n_filters'] * 2, kernel_size=(1, hparams['s_filters']), padding='same', kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.Conv2D(hparams['n_filters'] * 2, kernel_size=(1, hparams['s_filters']), padding='same', kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.MaxPool2D(pool_size=(1, 3), strides=(1, 2), padding='same'),\n",
    "    tw.keras.layers.Dropout(hparams['dropout']),\n",
    "\n",
    "    tw.keras.layers.Conv2D(hparams['n_filters'] * 4, kernel_size=(1, hparams['s_filters']), padding='same', kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.Conv2D(hparams['n_filters'] * 4, kernel_size=(1, hparams['s_filters']), padding='same', kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.MaxPool2D(pool_size=(1, 3), strides=(1, 2), padding='same'),\n",
    "    tw.keras.layers.Dropout(hparams['dropout']),\n",
    "\n",
    "    tw.keras.layers.Flatten(),\n",
    "\n",
    "    tw.keras.layers.Dense(hparams['n_units'], kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.Dropout(hparams['dropout']),\n",
    "\n",
    "    tw.keras.layers.Dense(hparams['n_units'] * 2, kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.Dropout(hparams['dropout']),\n",
    "\n",
    "    tw.keras.layers.Dense(hparams['n_units'] * 2, kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.Dropout(hparams['dropout']),\n",
    "\n",
    "    tw.keras.layers.Dense(hparams['n_units'] * 2, kernel_initializer=hparams['m_init']),\n",
    "    tw.keras.layers.BatchNormalization(),\n",
    "    hparams['activ_fn'],\n",
    "    tw.keras.layers.Dense(20, 'softmax')\n",
    "  ], name='TCNN')\n",
    "\n",
    "  model_tcnn.compile(optimizer=hparams['optim'](),\n",
    "                    metrics=['categorical_accuracy'],\n",
    "                    loss='categorical_crossentropy')\n",
    "\n",
    "  return model_tcnn\n",
    "\n",
    "def epoch_graph_callback() -> tw.keras.callbacks.Callback:\n",
    "  return tw.keras.callbacks.TensorBoard(log_dir=ROOT_PATH / 'logs',\n",
    "                                        write_graph=True)\n",
    "\n",
    "def gpu_utiliz_callback() -> tw.keras.callbacks.Callback:\n",
    "  return GpuStatsLogger(['/gpu:0'], memory_utilization=True, gpu_utilization=True)\n",
    "\n",
    "def periodic_rate_decrementer(start: int, period: int, factor: float) -> Callable[[int, float], float]:\n",
    "  def stabilize_lr(epoch: int, learn: float) -> float:\n",
    "    if epoch < start:\n",
    "      return learn\n",
    "    if (epoch - start) % period == 0:\n",
    "      return learn * factor\n",
    "    return learn\n",
    "  return stabilize_lr\n",
    "\n",
    "def lr_updater_callback() -> tw.keras.callbacks.Callback:\n",
    "  return tw.keras.callbacks.LearningRateScheduler(\n",
    "    verbose=1,\n",
    "    schedule=periodic_rate_decrementer(30, 10, 0.6)\n",
    "  )\n",
    "\n",
    "def fitting_callbacks() -> List[tw.keras.callbacks.Callback]:\n",
    "  return [\n",
    "    epoch_graph_callback(),\n",
    "    gpu_utiliz_callback(),\n",
    "    lr_updater_callback()\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Display periodic plots across each fold\n",
    "pt.figure()\n",
    "\n",
    "# Reformat to sklearn compatible format\n",
    "train_samples_ny = train_data_in.numpy().reshape(train_data_in.numpy().shape[0], -1)\n",
    "train_labels_ny = ny.expand_dims(ny.array(train_labels), axis=1)\n",
    "\n",
    "# Configure t_loop and model hparams\n",
    "tparams: TrainParams = {\n",
    "  'n_folds': 5,\n",
    "  'n_batch': 64,\n",
    "  'n_epochs': 150,\n",
    "}\n",
    "hparams: TCNNParams = {\n",
    "  'optim': lambda: Adam(learning_rate=2e-4, weight_decay=9e-6),\n",
    "  'm_init': tw.keras.initializers.GlorotNormal(seed=93),\n",
    "  'activ_fn': tw.keras.layers.Activation('relu'),\n",
    "  's_filters': 7,\n",
    "  'n_filters': 64,\n",
    "  'n_units': 724,\n",
    "  'dropout': 0.3,\n",
    "}\n",
    "\n",
    "# Train and validate using crossvalidation technique\n",
    "val_accy, val_loss = [], []\n",
    "trn_accy, trn_loss = [], []\n",
    "for i, (fold_train, fold_valid) in enumerate(KFold(shuffle=True, n_splits=tparams['n_folds']).split(train_samples_ny, train_labels_ny)):\n",
    "  # Show progress for each fold\n",
    "  pt.subplot(1, tparams['n_folds'], i + 1)\n",
    "  pt.title(f'Fold No. {i + 1} During {tparams[\"n_folds\"]}-Fold Cross Validation')\n",
    "  pt.legend(['train_accy', 'valid_accy'])\n",
    "  pt.ylabel('Accuracy')\n",
    "  pt.xlabel('Epoch')\n",
    "  pt.grid(True)\n",
    "\n",
    "  # Transform points into compatible integer indices\n",
    "  fold_train, fold_valid = tw.convert_to_tensor(fold_train, dtype=tw.int32), \\\n",
    "                           tw.convert_to_tensor(fold_valid, dtype=tw.int32)\n",
    "\n",
    "  # Create new model with random weights\n",
    "  model_tcnn = create_tcnn_model(hparams)\n",
    "\n",
    "  # Gather points for each step\n",
    "  train_samples_fold = tw.gather(train_data_in, fold_train)\n",
    "  valid_samples_fold = tw.gather(train_data_in, fold_valid)\n",
    "  train_labels_fold = tw.gather(train_labels_one_hot, fold_train)\n",
    "  valid_labels_fold = tw.gather(train_labels_one_hot, fold_valid)\n",
    "\n",
    "  # Learn on training data and predict on valid set\n",
    "  fold_history = model_tcnn.fit(train_samples_fold, train_labels_fold,\n",
    "                 validation_data=(valid_samples_fold, valid_labels_fold),\n",
    "                 batch_size=tparams['n_batch'], epochs=tparams['n_epochs'],\n",
    "                 workers=8, use_multiprocessing=True, initial_epoch=0,\n",
    "                 callbacks=fitting_callbacks())\n",
    "\n",
    "  # Save max valid & train accy for the current fold\n",
    "  val_accy.append(fold_history.history['val_categorical_accuracy'][-1])\n",
    "  val_loss.append(fold_history.history['val_loss'][-1])\n",
    "  trn_accy.append(fold_history.history['categorical_accuracy'][-1])\n",
    "  trn_loss.append(fold_history.history['loss'][-1])\n",
    "\n",
    "  # Display fold results\n",
    "  pt.plot(ny.arange(tparams['n_epochs']), fold_history.history['categorical_accuracy'])\n",
    "  pt.plot(ny.arange(tparams['n_epochs']), fold_history.history['val_categorical_accuracy'])\n",
    "\n",
    "# Show results across all folds\n",
    "pt.show()\n",
    "pt.figure()\n",
    "print(f'mean_val_loss = {ny.mean(val_loss)}')\n",
    "print(f'mean_val_accy = {ny.mean(val_accy)}')\n",
    "print(f'mean_trn_loss = {ny.mean(trn_loss)}')\n",
    "print(f'mean_trn_accy = {ny.mean(trn_accy)}')\n",
    "\n",
    "# Show val across folds\n",
    "pt.subplot(1, 2, 1)\n",
    "pt.grid(True)\n",
    "pt.xlabel('nth Fold')\n",
    "pt.ylabel('Accuracy')\n",
    "pt.title('Validation Accuracy')\n",
    "pt.plot(ny.arange(tparams['n_folds']), val_accy)\n",
    "\n",
    "# Show train across folds\n",
    "pt.subplot(1, 2, 2)\n",
    "pt.grid(True)\n",
    "pt.xlabel('nth Fold')\n",
    "pt.ylabel('Accuracy')\n",
    "pt.title('Training Accuracy')\n",
    "pt.plot(ny.arange(tparams['n_folds']), trn_accy)\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "# Recreate model to train from scratch for subtask\n",
    "model_tcnn = create_tcnn_model(hparams)\n",
    "\n",
    "# Split data indices\n",
    "n_subset_valid = 2_000\n",
    "n_subset_train_i = train_data_in.shape[0] - n_subset_valid\n",
    "n_subset_valid_i = n_subset_train_i\n",
    "\n",
    "# Fit model on all training data except last 2_000 entries\n",
    "model_tcnn.fit(train_data_in[:n_subset_train_i], train_labels_one_hot[:n_subset_train_i],\n",
    "               batch_size=tparams['n_batch'], epochs=tparams['n_epochs'],\n",
    "               initial_epoch=0, workers=8, use_multiprocessing=True,\n",
    "               callbacks=[lr_updater_callback()])\n",
    "\n",
    "# Predict on last 2_000 entries\n",
    "y_pred = model_tcnn.predict(train_data_in[n_subset_valid_i:])\n",
    "\n",
    "# Transform back to labels instead of probs\n",
    "y_pred_labels = ny.argmax(y_pred, axis=1)\n",
    "y_grnd_labels = ny.argmax(train_labels_one_hot[n_subset_valid_i:], axis=1)\n",
    "m_cnf = confusion_matrix(y_grnd_labels, y_pred_labels)\n",
    "ConfusionMatrixDisplay(m_cnf).plot()\n",
    "pt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model for final prediction on test set\n",
    "model_tcnn = create_tcnn_model(hparams)\n",
    "\n",
    "# Fit on the entire training to have more data for further predictions\n",
    "model_tcnn.fit(train_data_in, train_labels_one_hot,\n",
    "               batch_size=tparams['n_batch'], epochs=tparams['n_epochs'],\n",
    "               initial_epoch=0, workers=8, use_multiprocessing=True,\n",
    "               callbacks=[lr_updater_callback()])\n",
    "\n",
    "# Predict unknown labels\n",
    "test_pred_probs = model_tcnn.predict(test_data_in, batch_size=tparams['n_batch'],\n",
    "                                      workers=8, use_multiprocessing=True)\n",
    "test_pred_labels = ny.argmax(test_pred_probs, axis=1) + 1 # [0, 19] -> [1, 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resulting test object\n",
    "test_ids = ps.Series([p.stem for p in sorted(DATASET_TEST_PATH.glob('*.csv'))], name='id')\n",
    "test_classes = ps.Series(test_pred_labels, name='class')\n",
    "test_results = ps.DataFrame({\n",
    "  'id': test_ids,\n",
    "  'class': test_classes,\n",
    "})\n",
    "\n",
    "# And store the results obtained by the model using that previously created test frame\n",
    "test_results.to_csv(SUBMISSIONS_PATH / 'test_labels_11.csv', mode='w', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ec9c3918d717d506ab2e65d6c65aa7216d7534a07807507f90956365b2b23fa0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
